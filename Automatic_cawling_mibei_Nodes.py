# å¯¼å…¥å¿…è¦çš„æ ‡å‡†åº“æ¨¡å—
import os
import re
import sys
import time
import random
import requests
import subprocess
import psutil
import json
import base64
import socket
import struct
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from typing import Optional, List, Dict, Any, Set

# === é«˜çº§é»‘å®¢æ¨¡å—å¯¼å…¥ ===
try:
    import aiohttp
    import asyncio
    import aiofiles
    has_async = True
    
    class ConnectionPool:
        """é«˜æ•ˆè¿æ¥æ± ç®¡ç†"""
        
        def __init__(self, max_connections: int = 100):
            self.max_connections = max_connections
            self.semaphore = asyncio.Semaphore(max_connections)
            self.session = None
            
        async def __aenter__(self):
            if self.session is None:
                self.session = aiohttp.ClientSession()
            return self
        
        async def __aexit__(self, exc_type, exc_val, exc_tb):
            if self.session:
                await self.session.close()
                self.session = None
        
        async def acquire(self):
            """è·å–è¿æ¥èµ„æº"""
            await self.semaphore.acquire()
            return self.session
        
        def release(self):
            """é‡Šæ”¾è¿æ¥èµ„æº"""
            self.semaphore.release()
except ImportError:
    logging.warning("ğŸš« å¼‚æ­¥æ¨¡å—æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŒæ­¥æ¨¡å¼è¿è¡Œ")
    has_async = False

try:
    import win32api
    import win32process
    import win32con
    has_win32 = True
except ImportError:
    logging.warning("ğŸš« win32æ¨¡å—æœªå®‰è£…ï¼Œè¿›ç¨‹éšè—åŠŸèƒ½å—é™")
    has_win32 = False


# === é…ç½®ç±» ===
class Config:
    """ç¨‹åºå…¨å±€é…ç½®ç±»"""
    
    if getattr(sys, 'frozen', False):
        BASE_DIR = os.path.dirname(os.path.abspath(sys.executable))
    else:
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    
    V2RAYN_EXE = "v2rayN.exe"
    CONFIG_FILE = "config.json"
    NODE_KING_FILE = "config.json"
    NODES_FILE = "nodes.txt"
    CHECK_TIMEOUT = 10
    MAIN_URL = 'https://www.mibei77.com/'
    
    CONFIG_PATHS = [
        os.path.join(BASE_DIR, CONFIG_FILE),
        os.path.join(BASE_DIR, "binConfigs", CONFIG_FILE),
        os.path.join(os.path.expanduser("~"), "v2rayN", CONFIG_FILE),
        os.path.join(BASE_DIR, "v2rayN", CONFIG_FILE),
        os.path.join(BASE_DIR, "config", CONFIG_FILE),
    ]
    
    MAX_CONCURRENT_REQUESTS = 20
    CONNECTION_TIMEOUT = 10
    RETRY_ATTEMPTS = 3
    
    ENABLE_STEALTH = True
    ENABLE_FAKE_LOGGING = True
    MIN_DELAY = 1.0
    MAX_DELAY = 3.0
    
    ENABLE_NODE_BENCHMARK = True
    BENCHMARK_THRESHOLD = 1000
    TOP_NODES_PERCENTAGE = 20
    
    MAX_NODES = 250
    ENABLE_NODE_FILTERING = True
    ENABLE_SPEED_TEST = True
    MAX_LATENCY = 1000
    IGNORE_LATENCY_TEST = False
    
    ENABLE_ADVANCED_STEALTH = True
    RANDOMIZE_FILENAMES = True
    CLEANUP_TEMP_FILES = True
    
    MAX_MEMORY_USAGE = 512
    ENABLE_AUTO_OPTIMIZE = True
    
    ENABLE_DEBUG_LOGGING = False
    LOG_SENSITIVE_INFO = False
    
    # ğŸ† èŠ‚ç‚¹ç‹æ®‹é…·æ·˜æ±°é…ç½® (æ–°å¢)
    NODE_KING_ENABLED = True
    KING_MAX_DAYS = 7
    NODE_INACTIVE_DAYS = 3
    MAX_CONSECUTIVE_FAILS = 3
    MIN_SUCCESS_RATE = 0.7
    SCORE_THRESHOLD = 60
    
    # ğŸ‘‘ å†å²èŠ‚ç‚¹ç‹é…ç½® (æ–°å¢)
    HISTORY_KING_ENABLED = True
    HISTORY_KING_MIN_SCORE = 70  # å†å²èŠ‚ç‚¹ç‹æœ€ä½å¾—åˆ†
    MAX_KING_INACTIVE_DAYS = 14  # å†å²èŠ‚ç‚¹ç‹æœ€å¤§æœªæ´»è·ƒå¤©æ•°
    ENABLE_KING_REVIVAL = True   # æ˜¯å¦å…è®¸é‡æ–°æ¿€æ´»å†å²èŠ‚ç‚¹ç‹
    KING_REVIVAL_SCORE_BOOST = 1.2  # å†å²èŠ‚ç‚¹ç‹é‡æ–°æ¿€æ´»æ—¶çš„å¾—åˆ†åŠ æˆ
    
    # âš¡ æµ‹é€Ÿä¼˜åŒ–é…ç½® (æ–°å¢)
    TEST_TIMEOUT_MIN = 1.0
    TEST_TIMEOUT_MAX = 2.5
    MAX_TEST_LATENCY = 2000
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/113.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/114.0.1823.67 Safari/537.36",
        "Mozilla/5.0 (Linux; Android 13; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36"
    ]
    
    FULL_HEADERS = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0',
        'TE': 'trailers',
        'Pragma': 'no-cache'
    }
    
    FAKE_LOG_MESSAGES = [
        "æ­£åœ¨æ£€æŸ¥ç³»ç»Ÿæ›´æ–°...",
        "æ¸…ç†ä¸´æ—¶æ–‡ä»¶ä¸­...",
        "ä¼˜åŒ–ç½‘ç»œè®¾ç½®...",
        "æ‰«æç³»ç»Ÿå®‰å…¨...",
        "å¤‡ä»½ç”¨æˆ·æ•°æ®...",
        "æ ¡å‡†ç³»ç»Ÿæ—¶é—´...",
        "åŒæ­¥ç½‘ç»œé…ç½®...",
        "æ£€æŸ¥ç¡¬ä»¶çŠ¶æ€..."
    ]


# === æ—¥å¿—è®¾ç½® ===
def setup_logging():
    """é…ç½®æ—¥å¿—è®°å½•ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(os.path.join(Config.BASE_DIR, 'v2ray_updater.log'), encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    if sys.stdout.encoding != 'utf-8':
        sys.stdout.reconfigure(encoding='utf-8')


# === èŠ‚ç‚¹ç‹æ®‹é…·æ·˜æ±°ç³»ç»Ÿ ===
class NodeKingSystem:
    """èŠ‚ç‚¹ç‹æ®‹é…·æ·˜æ±°ç³»ç»Ÿ - æœ€å°å…¥ä¾µç‰ˆ"""
    
    def __init__(self, data_file: str = None):
        self.data_file = os.path.join(Config.BASE_DIR, data_file or Config.NODE_KING_FILE)
        self.nodes = {}      # æ´»è·ƒèŠ‚ç‚¹ {node_id: data}
        self.kings = {}      # èŠ‚ç‚¹ç‹è®°å½•
        self.dead = {}       # æ·˜æ±°èŠ‚ç‚¹
        self._load()
    
    def _load(self):
        """åŠ è½½æ•°æ®"""
        try:
            if os.path.exists(self.data_file):
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.nodes = data.get('nodes', {})
                    self.kings = data.get('kings', {})
                    self.dead = data.get('dead', {})
                self._clean_old()
        except Exception:
            self.nodes = {}
            self.kings = {}
            self.dead = {}
    
    def save(self):
        """ä¿å­˜æ•°æ®"""
        try:
            data = {
                'nodes': self.nodes,
                'kings': self.kings,
                'dead': self.dead,
                'update_time': datetime.now().isoformat()
            }
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logging.error(f"[ä¿å­˜å¤±è´¥] {e}")
    
    def _clean_old(self):
        """æ¸…ç†30å¤©å‰çš„æ·˜æ±°è®°å½•"""
        cutoff = time.time() - 30 * 86400
        to_remove = [nid for nid, data in self.dead.items() 
                    if data.get('death_time', 0) < cutoff]
        
        for nid in to_remove:
            del self.dead[nid]
        
        if to_remove:
            logging.debug(f"[æ¸…ç†] åˆ é™¤{len(to_remove)}æ¡æ—§è®°å½•")
    
    def get_id(self, node_str: str) -> str:
        """ç”ŸæˆèŠ‚ç‚¹ID"""
        import hashlib
        content = node_str.strip().replace('\n', '').replace('\r', '').replace(' ', '')
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def _create_data(self, node_str: str) -> dict:
        """åˆ›å»ºèŠ‚ç‚¹æ•°æ®"""
        return {
            'node': node_str,
            'create_time': datetime.now().isoformat(),
            'tests': 0,
            'success': 0,
            'fails': 0,
            'consecutive_fails': 0,
            'total_latency': 0,
            'latency_count': 0,
            'avg_latency': float('inf'),
            'best_latency': float('inf'),
            'worst_latency': 0,
            'success_rate': 0,
            'last_success': None,
            'last_fail': None,
            'last_active': datetime.now().isoformat(),
            'age_days': 0,
            'king_days': 0,
            'score': 50,
            'status': 'normal'
        }
    
    def update(self, node_str: str, latency: float, success: bool):
        """æ›´æ–°èŠ‚ç‚¹çŠ¶æ€"""
        node_id = self.get_id(node_str)
        
        if node_id in self.dead:
            return
        
        if node_id not in self.nodes:
            self.nodes[node_id] = self._create_data(node_str)
        
        data = self.nodes[node_id]
        data['tests'] += 1
        
        if success:
            data['success'] += 1
            data['consecutive_fails'] = 0
            data['last_success'] = datetime.now().isoformat()
            
            if latency < float('inf'):
                data['total_latency'] += latency
                data['latency_count'] += 1
                data['avg_latency'] = data['total_latency'] / data['latency_count']
                data['best_latency'] = min(data['best_latency'], latency)
                data['worst_latency'] = max(data['worst_latency'], latency)
        else:
            data['fails'] += 1
            data['consecutive_fails'] += 1
            data['last_fail'] = datetime.now().isoformat()
        
        if data['tests'] > 0:
            data['success_rate'] = data['success'] / data['tests']
        
        data['last_active'] = datetime.now().isoformat()
        create_time = datetime.fromisoformat(data['create_time'])
        data['age_days'] = (datetime.now() - create_time).days
        
        self._check_eliminate(node_id, data)
    
    def _check_eliminate(self, node_id: str, data: dict):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ·˜æ±°"""
        eliminate = False
        reason = ""
        
        if data['consecutive_fails'] >= Config.MAX_CONSECUTIVE_FAILS:
            eliminate = True
            reason = f"è¿ç»­å¤±è´¥{data['consecutive_fails']}æ¬¡"
        
        elif data['tests'] >= 10 and data['success_rate'] < Config.MIN_SUCCESS_RATE:
            eliminate = True
            reason = f"æˆåŠŸç‡{data['success_rate']:.1%}è¿‡ä½"
        
        elif data.get('status') == 'king' and data['king_days'] >= Config.KING_MAX_DAYS:
            eliminate = True
            reason = f"èŠ‚ç‚¹ç‹åœ¨ä½{data['king_days']}å¤©åˆ°æœŸ"
        
        elif self._days_inactive(data['last_active']) >= Config.NODE_INACTIVE_DAYS:
            eliminate = True
            reason = f"è¿ç»­{self._days_inactive(data['last_active'])}å¤©æœªæ´»è·ƒ"
        
        if eliminate:
            self._eliminate(node_id, data, reason)
    
    def _days_inactive(self, time_str: str) -> int:
        """è®¡ç®—æœªæ´»è·ƒå¤©æ•°"""
        try:
            if not time_str:
                return 999
            last = datetime.fromisoformat(time_str)
            return (datetime.now() - last).days
        except:
            return 999
    
    def _eliminate(self, node_id: str, data: dict, reason: str):
        """æ·˜æ±°èŠ‚ç‚¹"""
        if data.get('status') == 'king':
            # ä¿å­˜åˆ°å†å²èŠ‚ç‚¹ç‹è®°å½•
            self.kings[node_id] = {
                'node': data['node'],
                'king_days': data['king_days'],
                'best_latency': data['best_latency'],
                'avg_latency': data['avg_latency'],
                'worst_latency': data['worst_latency'],
                'success_rate': data['success_rate'],
                'score': data['score'],
                'end_time': datetime.now().isoformat(),
                'reason': reason,
                'last_active': data['last_active']
            }
        
        self.dead[node_id] = {
            **data,
            'death_time': time.time(),
            'death_reason': reason
        }
        
        del self.nodes[node_id]
        logging.info(f"[æ·˜æ±°] {node_id[:8]}: {reason}")
    
    def _calculate_score(self, data: dict) -> float:
        """è®¡ç®—èŠ‚ç‚¹å¾—åˆ†"""
        success_score = data['success_rate'] * 40 if data['tests'] > 0 else 0
        
        speed_score = 0
        if data['avg_latency'] < float('inf'):
            if data['avg_latency'] <= 100:
                speed_score = 30
            elif data['avg_latency'] <= 500:
                speed_score = 30 * (1 - (data['avg_latency'] - 100) / 400)
        
        stability_score = 0
        if (data['best_latency'] < float('inf') and 
            data['worst_latency'] > 0 and
            data['worst_latency'] - data['best_latency'] <= 100):
            stability_score = 20
        elif (data['best_latency'] < float('inf') and 
              data['worst_latency'] > 0):
            latency_range = data['worst_latency'] - data['best_latency']
            if latency_range <= 300:
                stability_score = 20 * (1 - (latency_range - 100) / 200)
        
        persistence_score = min(10, data['age_days'])
        penalty = data['consecutive_fails'] * 5
        score = max(0, success_score + speed_score + stability_score + persistence_score - penalty)
        data['score'] = score
        
        return score
    
    def select_king(self) -> Optional[dict]:
        """é€‰æ‹©èŠ‚ç‚¹ç‹"""
        if not self.nodes:
            return None
        
        candidates = []
        for node_id, data in self.nodes.items():
            score = self._calculate_score(data)
            
            if (score < Config.SCORE_THRESHOLD or 
                data['consecutive_fails'] > 0 or
                (data['tests'] >= 5 and data['success_rate'] < 0.8)):
                continue
            
            candidates.append({
                'node_id': node_id,
                'data': data,
                'score': score
            })
        
        if not candidates:
            return None
        
        candidates.sort(key=lambda x: x['score'], reverse=True)
        winner = candidates[0]
        node_id = winner['node_id']
        data = winner['data']
        
        old_kings = [nid for nid, ndata in self.nodes.items() 
                    if ndata.get('status') == 'king']
        for old_id in old_kings:
            self.nodes[old_id]['status'] = 'normal'
            self.nodes[old_id]['king_days'] = 0
        
        self.nodes[node_id]['status'] = 'king'
        self.nodes[node_id]['king_days'] = self.nodes[node_id].get('king_days', 0) + 1
        
        self.kings[node_id] = {
            'node': data['node'],
            'score': winner['score'],
            'avg_latency': data['avg_latency'],
            'success_rate': data['success_rate'],
            'age_days': data['age_days'],
            'start_time': datetime.now().isoformat(),
            'best_latency': data['best_latency'],
            'worst_latency': data['worst_latency'],
            'last_active': datetime.now().isoformat()
        }
        
        logging.info(f"[èŠ‚ç‚¹ç‹] {node_id[:8]} å¾—åˆ†:{winner['score']:.1f} å»¶è¿Ÿ:{data['avg_latency']:.1f}ms")
        
        return {
            'node': data['node'],
            'node_id': node_id,
            'score': winner['score'],
            'latency': data['avg_latency']
        }
    
    def get_king(self) -> Optional[dict]:
        """è·å–å½“å‰èŠ‚ç‚¹ç‹"""
        for node_id, data in self.nodes.items():
            if data.get('status') == 'king':
                return {
                    'node': data['node'],
                    'node_id': node_id,
                    'score': data['score'],
                    'latency': data['avg_latency'],
                    'king_days': data['king_days']
                }
        return None
    
    def daily_check(self):
        """æ¯æ—¥æ£€æŸ¥"""
        logging.info("[æ¯æ—¥æ£€æŸ¥] å¼€å§‹æ‰§è¡Œ")
        
        to_eliminate = []
        now = datetime.now()
        
        for node_id, data in self.nodes.items():
            last_active = data.get('last_active')
            if last_active:
                try:
                    last = datetime.fromisoformat(last_active)
                    inactive_days = (now - last).days
                    
                    if inactive_days >= Config.NODE_INACTIVE_DAYS:
                        to_eliminate.append((node_id, data, f"è¿ç»­{inactive_days}å¤©æœªæ´»è·ƒ"))
                except:
                    pass
            
            if data.get('status') == 'king':
                data['king_days'] = data.get('king_days', 0) + 1
        
        for node_id, data, reason in to_eliminate:
            self._eliminate(node_id, data, reason)
        
        self.save()
        
        king_count = len([n for n in self.nodes.values() if n.get('status') == 'king'])
        total = len(self.nodes)
        dead = len(self.dead)
        
        logging.info(f"[æ¯æ—¥æ£€æŸ¥] å®Œæˆ: {king_count}ä¸ªèŠ‚ç‚¹ç‹, {total}ä¸ªæ´»è·ƒèŠ‚ç‚¹, {dead}ä¸ªæ·˜æ±°èŠ‚ç‚¹")
    
    def stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        kings = [n for n in self.nodes.values() if n.get('status') == 'king']
        
        avg_latency = float('inf')
        avg_success = 0
        
        if self.nodes:
            latencies = [n['avg_latency'] for n in self.nodes.values() 
                        if n['avg_latency'] < float('inf')]
            successes = [n['success_rate'] for n in self.nodes.values()]
            
            if latencies:
                avg_latency = sum(latencies) / len(latencies)
            if successes:
                avg_success = sum(successes) / len(successes)
        
        return {
            'active_nodes': len(self.nodes),
            'kings': len(kings),
            'dead_nodes': len(self.dead),
            'avg_latency': avg_latency,
            'avg_success': avg_success,
            'oldest_node': max([n['age_days'] for n in self.nodes.values()], default=0)
        }
    
    def _is_king_still_valid(self, king_id: str, king_data: dict) -> bool:
        """æ£€æŸ¥å†å²èŠ‚ç‚¹ç‹æ˜¯å¦ä»ç„¶æœ‰æ•ˆ - æ–°å¢æ–¹æ³•"""
        if not Config.HISTORY_KING_ENABLED:
            return False
        
        try:
            # 1. æ£€æŸ¥èŠ‚ç‚¹å­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆ
            node_str = king_data.get('node', '')
            if not node_str:
                return False
            
            # 2. æ£€æŸ¥æ˜¯å¦å·²è¢«æ·˜æ±°
            if king_id in self.dead:
                return False
            
            # 3. æ£€æŸ¥å¾—åˆ†é˜ˆå€¼
            score = king_data.get('score', 0)
            if score < Config.HISTORY_KING_MIN_SCORE:
                return False
            
            # 4. æ£€æŸ¥æœ€è¿‘æ˜¯å¦æ´»è·ƒï¼ˆé¿å…è¿‡æ—¶çš„èŠ‚ç‚¹ç‹ï¼‰
            last_active = king_data.get('last_active')
            if last_active:
                try:
                    last_time = datetime.fromisoformat(last_active)
                    inactive_days = (datetime.now() - last_time).days
                    if inactive_days > Config.MAX_KING_INACTIVE_DAYS:
                        return False
                except:
                    return False
            else:
                return False
            
            # 5. æ£€æŸ¥å»¶è¿Ÿæ˜¯å¦ä»ç„¶ä¼˜ç§€
            latency = king_data.get('avg_latency', float('inf'))
            if latency > Config.MAX_TEST_LATENCY * 0.7:  # å†å²èŠ‚ç‚¹ç‹è¦æ±‚æ›´ä¸¥æ ¼
                return False
            
            return True
        except Exception as e:
            logging.debug(f"[å†å²èŠ‚ç‚¹ç‹æ£€æŸ¥] {king_id[:8]} æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def get_best_king_overall(self) -> Optional[dict]:
        """è·å–æ‰€æœ‰èŠ‚ç‚¹ç‹ä¸­æ€§èƒ½æœ€å¥½çš„ï¼ˆåŒ…æ‹¬å†å²å’Œå½“å‰ï¼‰- æ–°å¢æ–¹æ³•"""
        if not Config.HISTORY_KING_ENABLED:
            return self.get_king()
        
        best_king = None
        best_score = -1
        
        # 1. æ£€æŸ¥å½“å‰èŠ‚ç‚¹ç‹
        current_king = self.get_king()
        if current_king:
            best_king = current_king
            best_score = current_king['score']
            logging.debug(f"[å†å²èŠ‚ç‚¹ç‹å¯¹æ¯”] å½“å‰èŠ‚ç‚¹ç‹: {current_king['node_id'][:8]} å¾—åˆ†:{current_king['score']:.1f}")
        
        # 2. æ£€æŸ¥å†å²èŠ‚ç‚¹ç‹
        for king_id, king_data in self.kings.items():
            # æ£€æŸ¥å†å²èŠ‚ç‚¹ç‹æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if not self._is_king_still_valid(king_id, king_data):
                continue
            
            score = king_data.get('score', 0)
            latency = king_data.get('avg_latency', float('inf'))
            
            # å¯¹å†å²èŠ‚ç‚¹ç‹ç»™äºˆé¢å¤–åŠ åˆ†ï¼ˆå› ä¸ºå®ƒä»¬æ›¾ç»æ˜¯ç‹è€…ï¼‰
            if Config.ENABLE_KING_REVIVAL:
                score = score * Config.KING_REVIVAL_SCORE_BOOST
            
            # ç»¼åˆè¯„åˆ†ï¼šå¾—åˆ† + (100 - å»¶è¿Ÿ/10)
            latency_bonus = max(0, 100 - (latency / 10))
            composite_score = score + latency_bonus * 0.3
            
            logging.debug(f"[å†å²èŠ‚ç‚¹ç‹å¯¹æ¯”] å†å²èŠ‚ç‚¹ç‹: {king_id[:8]} åŸå§‹å¾—åˆ†:{king_data.get('score', 0):.1f} "
                         f"åŠ æˆå:{score:.1f} å»¶è¿Ÿ:{latency:.1f}ms ç»¼åˆå¾—åˆ†:{composite_score:.1f}")
            
            if composite_score > best_score:
                best_score = composite_score
                best_king = {
                    'node': king_data['node'],
                    'node_id': king_id,
                    'score': king_data.get('score', 0),
                    'latency': latency,
                    'is_history': True,
                    'king_data': king_data,
                    'composite_score': composite_score
                }
        
        # 3. å¦‚æœå†å²èŠ‚ç‚¹ç‹æ›´å¥½ï¼Œä¸”å…è®¸é‡æ–°æ¿€æ´»
        if best_king and best_king.get('is_history') and Config.ENABLE_KING_REVIVAL:
            logging.info(f"[ğŸ†] å†å²èŠ‚ç‚¹ç‹ {best_king['node_id'][:8]} æ¯”å½“å‰èŠ‚ç‚¹ç‹æ›´ä¼˜ç§€ "
                        f"(å¾—åˆ†:{best_king['composite_score']:.1f} vs {current_king['score'] if current_king else 0:.1f})")
            
            # é‡æ–°æ¿€æ´»å†å²èŠ‚ç‚¹ç‹
            self._revive_history_king(best_king['node_id'], best_king['king_data'])
            
            # æ›´æ–°è¿”å›ä¿¡æ¯
            best_king['is_revived'] = True
        
        return best_king
    
    def _revive_history_king(self, king_id: str, king_data: dict):
        """é‡æ–°æ¿€æ´»å†å²èŠ‚ç‚¹ç‹ - æ–°å¢æ–¹æ³•"""
        try:
            node_str = king_data.get('node', '')
            if not node_str:
                return
            
            # 1. å¦‚æœå†å²èŠ‚ç‚¹ç‹åœ¨æ·˜æ±°è®°å½•ä¸­ï¼Œç§»é™¤å®ƒ
            if king_id in self.dead:
                logging.info(f"[ğŸ”„] ä»æ·˜æ±°è®°å½•ä¸­æ¢å¤å†å²èŠ‚ç‚¹ç‹: {king_id[:8]}")
                del self.dead[king_id]
            
            # 2. æ·»åŠ åˆ°æ´»è·ƒèŠ‚ç‚¹ä¸­
            self.nodes[king_id] = {
                **self._create_data(node_str),
                'status': 'king',
                'score': king_data.get('score', 0),
                'avg_latency': king_data.get('avg_latency', float('inf')),
                'best_latency': king_data.get('best_latency', king_data.get('avg_latency', float('inf'))),
                'worst_latency': king_data.get('worst_latency', king_data.get('avg_latency', float('inf'))),
                'success_rate': king_data.get('success_rate', 0),
                'total_latency': king_data.get('avg_latency', 100) * 10,  # ä¼°ç®—æ€»å»¶è¿Ÿ
                'latency_count': 10,
                'king_days': 1,  # é‡æ–°å¼€å§‹è®¡ç®—åœ¨ä½å¤©æ•°
                'age_days': 0,   # é‡æ–°è®¡ç®—èŠ‚ç‚¹å¹´é¾„
                'create_time': datetime.now().isoformat(),
                'last_active': datetime.now().isoformat(),
                'tests': 10,     # ç»™äºˆä¸€å®šçš„æµ‹è¯•æ¬¡æ•°
                'success': int(king_data.get('success_rate', 0.8) * 10),
                'fails': int((1 - king_data.get('success_rate', 0.8)) * 10)
            }
            
            # 3. æ›´æ–°å†å²è®°å½•
            self.kings[king_id]['revived'] = True
            self.kings[king_id]['revive_time'] = datetime.now().isoformat()
            self.kings[king_id]['revive_count'] = self.kings[king_id].get('revive_count', 0) + 1
            
            logging.info(f"[ğŸ”„] å†å²èŠ‚ç‚¹ç‹ {king_id[:8]} å·²é‡æ–°æ¿€æ´»ï¼Œå»¶è¿Ÿ:{king_data.get('avg_latency', 'N/A')}ms "
                        f"æˆåŠŸç‡:{king_data.get('success_rate', 0):.1%}")
            
            # 4. ä¿å­˜æ›´æ”¹
            self.save()
            
        except Exception as e:
            logging.error(f"[âŒ] é‡æ–°æ¿€æ´»å†å²èŠ‚ç‚¹ç‹å¤±è´¥: {e}")


# === å·¥å…·å‡½æ•° ===
def fake_logging():
    """ç”Ÿæˆè¿·æƒ‘æ€§æ—¥å¿—"""
    if Config.ENABLE_FAKE_LOGGING and random.random() < 0.3:
        logging.info(random.choice(Config.FAKE_LOG_MESSAGES))

def get_stealth_headers() -> Dict[str, str]:
    """ç”Ÿæˆæ›´éšè”½çš„å®Œæ•´è¯·æ±‚å¤´"""
    headers = Config.FULL_HEADERS.copy()
    headers['User-Agent'] = random.choice(Config.USER_AGENTS)
    
    if random.random() < 0.5:
        headers['DNT'] = '1'
    if random.random() < 0.3:
        headers['Sec-Fetch-Dest'] = 'document'
        headers['Sec-Fetch-Mode'] = 'navigate'
        headers['Sec-Fetch-Site'] = 'none'
        headers['Sec-Fetch-User'] = '?1'
    
    return headers

def smart_retry(max_retries=Config.RETRY_ATTEMPTS):
    """æ›´å®Œå–„çš„æ™ºèƒ½é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    if Config.ENABLE_STEALTH and attempt > 0:
                        sleep_time = (2 ** attempt) + random.uniform(0, 1)
                        logging.info(f"[ğŸ”„] ç¬¬{attempt+1}æ¬¡é‡è¯•ï¼Œç­‰å¾… {sleep_time:.2f} ç§’...")
                        time.sleep(sleep_time)
                    
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt == max_retries - 1:
                        logging.error(f"[âŒ] æ‰€æœ‰ {max_retries} æ¬¡é‡è¯•éƒ½å¤±è´¥äº†: {e}")
                        raise
                    logging.warning(f"[âš ï¸] ç¬¬ {attempt+1} æ¬¡å°è¯•å¤±è´¥: {e}ï¼Œå‡†å¤‡é‡è¯•...")
            raise last_exception
        return wrapper
    return decorator

async def fetch_page_async(session, url, headers=None):
    """å¼‚æ­¥è·å–é¡µé¢å†…å®¹"""
    if headers is None:
        headers = get_stealth_headers() if Config.ENABLE_STEALTH else get_random_headers()
    
    try:
        if Config.ENABLE_STEALTH:
            await asyncio.sleep(random.uniform(Config.MIN_DELAY, Config.MAX_DELAY))
        
        async with session.get(url, headers=headers, timeout=Config.CONNECTION_TIMEOUT) as response:
            response.raise_for_status()
            return await response.text()
    except Exception as e:
        logging.error(f"[Ã—] å¼‚æ­¥è¯·æ±‚ {url} å¤±è´¥: {e}")
        return None

async def test_node_speed_async(node_info):
    """å¼‚æ­¥æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿ"""
    start_time = time.time()
    host = node_info.get('address', '')
    port = node_info.get('port', 443)
    
    try:
        reader, writer = await asyncio.wait_for(
            asyncio.open_connection(host, port), 
            timeout=3.0
        )
        writer.close()
        await writer.wait_closed()
        latency = (time.time() - start_time) * 1000
        logging.debug(f"èŠ‚ç‚¹ {host}:{port} å»¶è¿Ÿ: {latency:.2f}ms")
        return {**node_info, 'latency': latency}
    except Exception:
        return {**node_info, 'latency': float('inf')}

def generate_random_string(length: int) -> str:
    """ç”Ÿæˆéšæœºå­—ç¬¦ä¸²ç”¨äºæ··æ·†"""
    import string
    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))

def get_random_headers(stealth=False):
    """ä¿®å¤å‡½æ•°ç­¾åä¸ä¸€è‡´é—®é¢˜"""
    if stealth or Config.ENABLE_STEALTH:
        return get_stealth_headers()
    return {"User-Agent": random.choice(Config.USER_AGENTS)}

def create_ghost_process(cmd):
    """åˆ›å»ºå‡ ä¹ä¸å¯è§çš„è¿›ç¨‹"""
    startupinfo = subprocess.STARTUPINFO()
    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
    
    creationflags = subprocess.CREATE_NO_WINDOW
    
    if has_win32:
        startupinfo.wShowWindow = win32con.SW_HIDE
        creationflags |= (subprocess.IDLE_PRIORITY_CLASS | 
                          win32process.CREATE_BREAKAWAY_FROM_JOB)
    
    process = subprocess.Popen(
        cmd,
        startupinfo=startupinfo,
        creationflags=creationflags
    )
    return process

class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨ï¼Œé¿å…å†…å­˜æ³„æ¼"""
    def __init__(self, cleanup_threshold: int = 50, max_age_seconds: int = 1800):
        self.cleanup_threshold = cleanup_threshold
        self.max_age_seconds = max_age_seconds
        self.operation_count = 0
        self.last_cleanup_time = time.time()
    
    def auto_cleanup(self, force: bool = False):
        """è‡ªåŠ¨æ¸…ç†å†…å­˜"""
        self.operation_count += 1
        current_time = time.time()
        
        should_cleanup = force or \
                       self.operation_count >= self.cleanup_threshold or \
                       (current_time - self.last_cleanup_time) > self.max_age_seconds
        
        if should_cleanup:
            import gc
            import psutil
            
            before_mem = psutil.Process().memory_info().rss / 1024 / 1024
            collected = gc.collect()
            gc.garbage.clear()
            after_mem = psutil.Process().memory_info().rss / 1024 / 1024
            
            freed_mem = before_mem - after_mem
            if freed_mem > 0:
                logging.info(f"[ğŸ§¹] å†…å­˜æ¸…ç†å®Œæˆ: é‡Šæ”¾ {freed_mem:.2f} MB, å›æ”¶ {collected} ä¸ªå¯¹è±¡")
            
            self.operation_count = 0
            self.last_cleanup_time = current_time

memory_optimizer = MemoryOptimizer()

def find_config_file(config_name: str = "config.json", search_dirs: Optional[List[str]] = None, recursive: bool = True) -> Optional[str]:
    """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒé€’å½’æŸ¥æ‰¾"""
    default_dirs = [
        Config.BASE_DIR,
        os.path.join(Config.BASE_DIR, "binConfigs"),
        os.path.join(os.path.expanduser("~"), "v2rayN"),
        os.environ.get('PROGRAMFILES', ''),
        os.environ.get('PROGRAMFILES(X86)', ''),
    ]
    
    dirs_to_search = search_dirs if search_dirs else default_dirs
    dirs_to_search = [d for d in dirs_to_search if d and os.path.exists(d)]
    
    for search_dir in dirs_to_search:
        if recursive:
            for root, dirs, files in os.walk(search_dir):
                if config_name in files:
                    config_path = os.path.abspath(os.path.join(root, config_name))
                    logging.debug(f"[ğŸ”] åœ¨ {config_path} æ‰¾åˆ°é…ç½®æ–‡ä»¶")
                    return config_path
        else:
            config_path = os.path.abspath(os.path.join(search_dir, config_name))
            if os.path.exists(config_path):
                logging.debug(f"[ğŸ”] åœ¨ {config_path} æ‰¾åˆ°é…ç½®æ–‡ä»¶")
                return config_path
    
    logging.debug(f"[âŒ] æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_name}")
    return None

def resilient_execute(func, fallback_func=None, max_attempts=3):
    """å¼¹æ€§æ‰§è¡Œï¼Œè‡ªåŠ¨æ¢å¤"""
    for attempt in range(max_attempts):
        try:
            return func()
        except Exception as e:
            logging.warning(f"[ğŸ”„] ç¬¬{attempt+1}æ¬¡æ‰§è¡Œå¤±è´¥: {e}")
            if attempt == max_attempts - 1 and fallback_func:
                logging.info("[ğŸ†˜] å¯ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return fallback_func()
            time.sleep(2 ** attempt)
    return None

def safe_file_operations(file_path, operation="write", content=None):
    """å®‰å…¨çš„æ–‡ä»¶æ“ä½œï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±"""
    temp_path = file_path + ".tmp"
    
    try:
        if operation == "write" and content is not None:
            with open(temp_path, "w", encoding="utf-8") as f:
                f.write(content)
            if os.path.exists(file_path):
                os.remove(file_path)
            os.rename(temp_path, file_path)
            return True
            
        elif operation == "read":
            if os.path.exists(file_path):
                with open(file_path, "r", encoding="utf-8") as f:
                    return f.read()
            return None
            
    except Exception as e:
        if os.path.exists(temp_path):
            os.remove(temp_path)
        logging.error(f"[âŒ] æ–‡ä»¶æ“ä½œå¤±è´¥: {e}")
        return None
    
    return None

def get_v2rayn_path() -> str:
    """è·å–v2raynå¯æ‰§è¡Œæ–‡ä»¶å®Œæ•´è·¯å¾„"""
    platform = PlatformAdapter.get_platform()
    
    if platform == 'windows':
        return os.path.join(Config.BASE_DIR, Config.V2RAYN_EXE)
    else:
        return os.path.join(Config.BASE_DIR, 'v2rayn')

async def download_nodes_file_async(node_url):
    """å¼‚æ­¥ä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶"""
    fake_logging()
    logging.info(f"[ğŸ”’] æ­£åœ¨å¼‚æ­¥ä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶: {node_url}")
    
    if has_async:
        async with aiohttp.ClientSession() as session:
            content = await fetch_page_async(session, node_url)
            if content:
                lines = content.strip().split('\n')
                unique_lines = []
                seen_node_identifiers = set()
                
                for line in lines:
                    if not line.strip():
                        continue
                        
                    node_identifier = None
                    if line.startswith("vmess://"):
                        try:
                            vmess_content = line[8:]
                            padding = len(vmess_content) % 4
                            if padding:
                                vmess_content += '=' * (4 - padding)
                            vmess_json = json.loads(base64.b64decode(vmess_content).decode('utf-8', errors='ignore'))
                            address = vmess_json.get("add", "")
                            port = str(vmess_json.get("port", ""))
                            if address and port:
                                node_identifier = f"{address}:{port}"
                        except Exception:
                            pass
                    
                    if node_identifier and node_identifier not in seen_node_identifiers:
                        seen_node_identifiers.add(node_identifier)
                        unique_lines.append(line)
                    elif not node_identifier and line not in unique_lines:
                        unique_lines.append(line)
                
                unique_content = '\n'.join(unique_lines)
                return unique_content
    return None

def get_config_path(v2rayn_dir: Optional[str] = None) -> Optional[str]:
    """è·å–v2rayné…ç½®æ–‡ä»¶å®Œæ•´è·¯å¾„"""
    if v2rayn_dir:
        config_path = PlatformAdapter.get_config_path(v2rayn_dir, Config.CONFIG_FILE, search_subdirs=True)
        if config_path:
            return config_path
    
    config_path = find_config_file(Config.CONFIG_FILE)
    if config_path:
        return config_path
    
    for path in Config.CONFIG_PATHS:
        if os.path.exists(path):
            logging.debug(f"[âœ…] åœ¨é¢„å®šä¹‰è·¯å¾„æ‰¾åˆ°é…ç½®æ–‡ä»¶: {path}")
            return path
    
    logging.warning(f"[âŒ] æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {Config.CONFIG_FILE}")
    return None

def get_nodes_path() -> str:
    """è·å–èŠ‚ç‚¹ä¿¡æ¯æ–‡ä»¶ä¿å­˜è·¯å¾„"""
    return os.path.join(Config.BASE_DIR, Config.NODES_FILE)

def is_v2rayn_running() -> bool:
    """æ£€æŸ¥v2raynè¿›ç¨‹æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
    fake_logging()
    platform = PlatformAdapter.get_platform()
    
    for proc in psutil.process_iter(['name']):
        try:
            proc_name = proc.info['name']
            if not proc_name:
                continue
                
            if platform == 'windows':
                if 'v2rayn.exe' in proc_name.lower():
                    return True
            else:
                if proc_name.lower() == 'v2rayn':
                    return True
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    return False

def wait_for_v2rayn(timeout: int = Config.CHECK_TIMEOUT) -> bool:
    """ç­‰å¾…v2rayNå¯åŠ¨ï¼Œç›´åˆ°è¶…æ—¶"""
    fake_logging()
    logging.info(f"[âŒ›] ç­‰å¾…v2rayNå¯åŠ¨ï¼ˆæœ€å¤š {timeout} ç§’ï¼‰...")
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        if is_v2rayn_running():
            logging.info("[âœ…] v2rayN å·²å¯åŠ¨")
            return True
        sleep_time = random.uniform(0.8, 1.2)
        time.sleep(sleep_time)
    
    logging.warning("[âŒ] è¶…æ—¶æœªæ£€æµ‹åˆ° v2rayN è¿›ç¨‹")
    return False

def terminate_v2rayn() -> bool:
    """ç»ˆæ­¢æ­£åœ¨è¿è¡Œçš„v2rayNè¿›ç¨‹"""
    fake_logging()
    logging.info("[ğŸ”ª] å°è¯•å…³é—­æ—§çš„ v2rayN...")
    terminated = False
    
    for proc in psutil.process_iter(['name']):
        try:
            if proc.info['name'] and 'v2rayn.exe' in proc.info['name'].lower():
                try:
                    proc.terminate()
                    proc.wait(timeout=5)
                    terminated = True
                except psutil.TimeoutExpired:
                    logging.warning("[âš¡] è¿›ç¨‹è¶…æ—¶ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    proc.kill()
                    terminated = True
                except psutil.NoSuchProcess:
                    pass
                except psutil.AccessDenied:
                    logging.error("[ğŸš«] æ²¡æœ‰è¶³å¤Ÿæƒé™ç»ˆæ­¢è¿›ç¨‹")
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    
    time.sleep(random.uniform(0.5, 1.5))
    return terminated

def start_v2rayn() -> bool:
    """å¯åŠ¨v2raynç¨‹åº"""
    v2rayn_path = get_v2rayn_path()
    platform = PlatformAdapter.get_platform()
    
    if not os.path.exists(v2rayn_path):
        logging.error(f"[âŒ] v2rayn æ–‡ä»¶ä¸å­˜åœ¨: {v2rayn_path}")
        return False
    
    try:
        fake_logging()
        logging.info(f"[ğŸš€] æ­£åœ¨å¯åŠ¨ v2rayn (éšèº«æ¨¡å¼ï¼Œå¹³å°: {platform})...")
        
        if platform == 'windows':
            if Config.ENABLE_STEALTH and has_win32:
                create_ghost_process([v2rayn_path])
            else:
                startupinfo = subprocess.STARTUPINFO()
                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
                subprocess.Popen([v2rayn_path], startupinfo=startupinfo)
        else:
            os.chmod(v2rayn_path, 0o755)
            
            if Config.ENABLE_STEALTH:
                subprocess.Popen([v2rayn_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, stdin=subprocess.DEVNULL, close_fds=True)
            else:
                subprocess.Popen([v2rayn_path])
            
        time.sleep(random.uniform(0.5, 1.5))
        return wait_for_v2rayn()
    except Exception as e:
        logging.error(f"[âŒ] å¯åŠ¨ v2rayn å¤±è´¥: {e}")
        return False

def restart_v2rayn() -> bool:
    """é‡å¯v2rayNç¨‹åº"""
    terminate_v2rayn()
    return start_v2rayn()

@smart_retry(max_retries=3)
def update_v2rayn_subscription(new_url: str) -> bool:
    """æ›¿æ¢ v2rayN config.json çš„è®¢é˜…é“¾æ¥ä¸ºæ–°çš„ URL"""
    fake_logging()
    config_path = get_config_path()
    if not config_path or not os.path.exists(config_path):
        logging.error(f"[âŒ] æ‰¾ä¸åˆ° config.jsonï¼š{config_path}")
        return False
    
    try:
        time.sleep(random.uniform(0.1, 0.3))
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        if Config.ENABLE_STEALTH:
            config_data["lastUpdateTime"] = int(time.time() * 1000)
            config_data["autoUpdateCore"] = False
            config_data["logLevel"] = "none"
            config_data["guiType"] = 0
        
        subscription_remarks = "Auto Imported" if not Config.ENABLE_STEALTH else generate_random_string(8)
        config_data["subscriptions"] = [{"url": new_url, "enabled": True, "remarks": subscription_remarks}]
        
        time.sleep(random.uniform(0.1, 0.3))
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config_data, f, indent=4, ensure_ascii=False)
        
        masked_url = new_url[:10] + "..." + new_url[-10:] if len(new_url) > 20 else new_url
        logging.info(f"[âœ…] æˆåŠŸæ›¿æ¢è®¢é˜…é“¾æ¥: {masked_url}")
        return True
    
    except Exception as e:
        logging.error(f"[âŒ] æ›´æ–°è®¢é˜…å¤±è´¥: {type(e).__name__}: {e}")
        raise

def set_best_node_as_default(best_node: str, group_name: str = "ç±³è´") -> bool:
    """å°†æœ€ä¼˜èŠ‚ç‚¹è®¾ç½®ä¸ºv2rayNçš„é»˜è®¤èŠ‚ç‚¹"""
    fake_logging()
    
    v2rayn_dir = find_v2rayn_installation()
    if not v2rayn_dir:
        logging.info("[â„¹ï¸] æ‰¾ä¸åˆ°v2rayNå®‰è£…ç›®å½•ï¼Œè·³è¿‡è®¾ç½®é»˜è®¤èŠ‚ç‚¹æ­¥éª¤")
        return True
    
    config_path = get_config_path(v2rayn_dir)
    if not config_path or not os.path.exists(config_path):
        logging.info("[â„¹ï¸] æ‰¾ä¸åˆ°config.jsonæ–‡ä»¶ï¼Œè·³è¿‡è®¾ç½®é»˜è®¤èŠ‚ç‚¹æ­¥éª¤")
        return True
    
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        if "servers" not in config_data:
            config_data["servers"] = []
        
        best_node_address = None
        best_node_port = None
        
        if best_node and best_node.startswith("vmess://"):
            try:
                vmess_content = best_node[8:]
                padding = len(vmess_content) % 4
                if padding:
                    vmess_content += '=' * (4 - padding)
                vmess_json = json.loads(base64.b64decode(vmess_content).decode('utf-8'))
                best_node_address = vmess_json.get("add", "")
                best_node_port = int(vmess_json.get("port", 443))
            except Exception as e:
                logging.error(f"[âŒ] è§£ææœ€ä¼˜èŠ‚ç‚¹å¤±è´¥: {str(e)}")
                return False
        
        best_node_index = -1
        for i, server in enumerate(config_data["servers"]):
            if server.get("group") == group_name and server.get("address") == best_node_address and server.get("port") == best_node_port:
                best_node_index = i
                break
        
        if best_node_index != -1:
            config_data["index"] = best_node_index
            logging.info(f"[ğŸ†] å·²å°†æœ€ä¼˜èŠ‚ç‚¹è®¾ç½®ä¸ºé»˜è®¤èŠ‚ç‚¹ï¼ˆç´¢å¼•: {best_node_index}ï¼‰")
            
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump(config_data, f, indent=4, ensure_ascii=False)
            
            return True
        else:
            logging.warning("[âš ï¸] åœ¨é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ€ä¼˜èŠ‚ç‚¹ï¼Œæ— æ³•è®¾ç½®ä¸ºé»˜è®¤èŠ‚ç‚¹")
            return False
            
    except json.JSONDecodeError as e:
        logging.error(f"[âŒ] è§£æé…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        logging.error(f"[âŒ] è®¾ç½®é»˜è®¤èŠ‚ç‚¹å¤±è´¥: {str(e)}")
        return False

def add_nodes_to_mibei_group(best_node: str = None) -> bool:
    """åœ¨v2rayNä¸­åˆ›å»ºåä¸º"ç±³è´"çš„åˆ†ç»„ï¼Œå¹¶å°†èŠ‚ç‚¹ç²˜è´´åˆ°è¯¥åˆ†ç»„ä¸­"""
    fake_logging()
    
    v2rayn_dir = find_v2rayn_installation()
    if not v2rayn_dir:
        logging.info("[â„¹ï¸] æ‰¾ä¸åˆ°v2rayNå®‰è£…ç›®å½•ï¼Œè·³è¿‡èŠ‚ç‚¹å¯¼å…¥æ­¥éª¤")
        return True
    
    config_path = get_config_path(v2rayn_dir)
    if not config_path or not os.path.exists(config_path):
        logging.info("[â„¹ï¸] æ‰¾ä¸åˆ°config.jsonæ–‡ä»¶ï¼Œè·³è¿‡èŠ‚ç‚¹å¯¼å…¥æ­¥éª¤")
        return True
    
    nodes_path = get_nodes_path()
    if not os.path.exists(nodes_path):
        logging.error(f"[âŒ] æ‰¾ä¸åˆ°èŠ‚ç‚¹æ–‡ä»¶: {nodes_path}")
        return False
    
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        with open(nodes_path, "r", encoding="utf-8") as f:
            node_lines = f.readlines()
        
        if Config.ENABLE_NODE_FILTERING:
            logging.info("[ğŸ§ ] æ­£åœ¨ç­›é€‰é«˜è´¨é‡èŠ‚ç‚¹...")
            if len(node_lines) > Config.MAX_NODES:
                node_lines = random.sample(node_lines, Config.MAX_NODES)
            logging.info(f"[âœ…] å·²ç­›é€‰å‡º {len(node_lines)} ä¸ªèŠ‚ç‚¹")
        
        if "servers" not in config_data:
            config_data["servers"] = []
        
        group_name = "ç±³è´" if not Config.ENABLE_STEALTH else f"ç±³è´_{generate_random_string(4)}"
        
        old_nodes = [server for server in config_data["servers"] if server.get("group") == "ç±³è´"]
        config_data["servers"] = [server for server in config_data["servers"] if server.get("group") != "ç±³è´"]
        
        logging.info(f"[ğŸ§¹] å·²æ¸…é™¤ {len(old_nodes)} ä¸ªæ—§èŠ‚ç‚¹")
        
        new_server_count = 0
        for line in node_lines:
            line = line.strip()
            if not line:
                continue
                
            time.sleep(random.uniform(0.01, 0.05))
            
            try:
                if line.startswith("vmess://"):
                    vmess_content = line[8:]
                    padding = len(vmess_content) % 4
                    if padding:
                        vmess_content += '=' * (4 - padding)
                    
                    vmess_json = json.loads(base64.b64decode(vmess_content).decode('utf-8'))
                    
                    server = {
                        "id": str(random.randint(100000, 999999)),
                        "remarks": vmess_json.get("ps", f"èŠ‚ç‚¹_{generate_random_string(6)}"),
                        "group": group_name,
                        "type": "VMess",
                        "address": vmess_json.get("add", ""),
                        "port": int(vmess_json.get("port", 443)),
                        "uuid": vmess_json.get("id", ""),
                        "alterId": int(vmess_json.get("aid", 0)),
                        "security": vmess_json.get("scy", "auto"),
                        "network": vmess_json.get("net", "tcp"),
                        "headerType": vmess_json.get("type", "none"),
                        "requestHost": vmess_json.get("host", ""),
                        "path": vmess_json.get("path", ""),
                        "streamSecurity": vmess_json.get("tls", ""),
                        "sni": vmess_json.get("sni", ""),
                        "fingerprint": vmess_json.get("fp", ""),
                        "allowInsecure": True
                    }
                    
                    if Config.ENABLE_SPEED_TEST and vmess_json.get("add") and vmess_json.get("port"):
                        latency = test_latency(vmess_json.get("add"), int(vmess_json.get("port", 443)))
                        if latency < Config.MAX_LATENCY or Config.IGNORE_LATENCY_TEST:
                            config_data["servers"].append(server)
                            new_server_count += 1
                            if latency < float('inf'):
                                logging.debug(f"[ğŸš€] æ·»åŠ é«˜é€ŸèŠ‚ç‚¹: {latency:.2f}ms")
                    else:
                        config_data["servers"].append(server)
                        new_server_count += 1
                
                elif line.startswith("trojan://"):
                    server = {
                        "id": str(random.randint(100000, 999999)),
                        "remarks": f"Trojan_{generate_random_string(6)}",
                        "group": group_name,
                        "type": "Trojan",
                        "allowInsecure": True
                    }
                    config_data["servers"].append(server)
                    new_server_count += 1
                    
                elif line.startswith("ss://"):
                    server = {
                        "id": str(random.randint(100000, 999999)),
                        "remarks": f"SS_{generate_random_string(6)}",
                        "group": group_name,
                        "type": "Shadowsocks",
                    }
                    config_data["servers"].append(server)
                    new_server_count += 1
            except Exception as e:
                logging.warning(f"[âš ï¸] è§£æèŠ‚ç‚¹å¤±è´¥: {line[:30]}... {str(e)}")
                continue
        
        time.sleep(random.uniform(0.2, 0.5))
        
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config_data, f, indent=4, ensure_ascii=False)
        
        logging.info(f"[âœ…] æˆåŠŸå°†{new_server_count}ä¸ªèŠ‚ç‚¹æ·»åŠ åˆ° {group_name} åˆ†ç»„")
        
        if best_node:
            logging.info("[ğŸ†] æ­£åœ¨è®¾ç½®æœ€ä¼˜èŠ‚ç‚¹ä¸ºé»˜è®¤èŠ‚ç‚¹...")
            if set_best_node_as_default(best_node, group_name):
                logging.info("[âœ…] å·²æˆåŠŸå°†æœ€ä¼˜èŠ‚ç‚¹è®¾ç½®ä¸ºé»˜è®¤èŠ‚ç‚¹")
            else:
                logging.warning("[âš ï¸] è®¾ç½®æœ€ä¼˜èŠ‚ç‚¹ä¸ºé»˜è®¤èŠ‚ç‚¹å¤±è´¥")
        
        return True
        
    except json.JSONDecodeError as e:
        logging.error(f"[âŒ] è§£æé…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        logging.error(f"[âŒ] æ·»åŠ èŠ‚ç‚¹åˆ°ç±³è´åˆ†ç»„å¤±è´¥: {type(e).__name__}: {e}")
        return False

def test_latency(host: str, port: int = 443, timeout: float = 1.0) -> float:
    """TCP pingæµ‹è¯•ï¼Œè¿”å›æ¯«ç§’å»¶è¿Ÿ"""
    try:
        time.sleep(random.uniform(0.001, 0.005))
        start = time.time()
        sock = socket.create_connection((host, port), timeout)
        sock.close()
        latency = (time.time() - start) * 1000
        logging.debug(f"[ğŸ“Š] èŠ‚ç‚¹å»¶è¿Ÿ: {host}:{port} = {latency:.2f}ms")
        return latency
    except socket.timeout:
        logging.debug(f"[â°] èŠ‚ç‚¹è¶…æ—¶: {host}:{port}")
        return float("inf")
    except Exception as e:
        logging.debug(f"[âŒ] å»¶è¿Ÿæµ‹è¯•å¤±è´¥: {host}:{port} - {str(e)}")
        return float("inf")

async def test_latency_async(host: str, port: int = 443, timeout: float = 1.0) -> float:
    """å¼‚æ­¥TCP pingæµ‹è¯•ï¼Œè¿”å›æ¯«ç§’å»¶è¿Ÿ"""
    if not has_async:
        return test_latency(host, port, timeout)
        
    try:
        await asyncio.sleep(random.uniform(0.001, 0.005))
        start = time.time()
        reader, writer = await asyncio.wait_for(
            asyncio.open_connection(host, port),
            timeout=timeout
        )
        writer.close()
        await writer.wait_closed()
        latency = (time.time() - start) * 1000
        logging.debug(f"[âš¡] å¼‚æ­¥èŠ‚ç‚¹å»¶è¿Ÿ: {host}:{port} = {latency:.2f}ms")
        return latency
    except Exception:
        return float("inf")

async def benchmark_nodes_async(nodes):
    """å¹¶å‘æµ‹é€Ÿæ‰€æœ‰èŠ‚ç‚¹ï¼Œè¿”å›æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨å’Œæœ€ä¼˜èŠ‚ç‚¹"""
    if not has_async:
        return nodes[:min(len(nodes), Config.MAX_NODES)], None
        
    async def process_node(node):
        if node.startswith("vmess://"):
            try:
                vmess_content = node[8:]
                padding = len(vmess_content) % 4
                if padding:
                    vmess_content += '=' * (4 - padding)
                vmess_json = json.loads(base64.b64decode(vmess_content).decode('utf-8', errors='ignore'))
                host = vmess_json.get("add", "")
                port = int(vmess_json.get("port", 443))
                if host and port:
                    latency = await test_latency_async(host, port)
                    if latency < Config.MAX_LATENCY:
                        return latency, node
            except Exception:
                pass
        return None, None
    
    semaphore = asyncio.Semaphore(Config.MAX_CONCURRENT_REQUESTS)
    
    async def bounded_process_node(node):
        async with semaphore:
            return await process_node(node)
    
    node_tasks = [bounded_process_node(node) for node in nodes]
    task_results = await asyncio.gather(*node_tasks)
    
    results = [(latency, node) for latency, node in task_results if latency is not None]
    results.sort(key=lambda x: x[0])
    
    top_count = min(len(results), Config.MAX_NODES)
    top_nodes = [node for _, node in results[:top_count]]
    
    best_node = None
    if results:
        best_latency, best_node = results[0]
        logging.info(f"[ğŸ†] æ‰¾åˆ°æœ€ä¼˜èŠ‚ç‚¹ï¼Œå»¶è¿Ÿ: {best_latency:.2f}ms")
    
    del task_results
    import gc
    gc.collect()
    
    logging.info(f"[ğŸ¯] å·²ä»{len(nodes)}ä¸ªèŠ‚ç‚¹ä¸­ç­›é€‰å‡º{len(top_nodes)}ä¸ªä½å»¶è¿ŸèŠ‚ç‚¹")
    return top_nodes, best_node

async def enhanced_benchmark_nodes_async(nodes: List[str], king_system: NodeKingSystem = None) -> tuple:
    """å¢å¼ºç‰ˆèŠ‚ç‚¹æµ‹é€Ÿ - é›†æˆèŠ‚ç‚¹ç‹æœºåˆ¶ï¼ˆä½¿ç”¨æœ€ä½³èŠ‚ç‚¹ç‹ï¼‰"""
    if not nodes:
        return [], None
    
    use_king_system = Config.NODE_KING_ENABLED and king_system is not None
    
    async def test_node(node: str):
        """æµ‹è¯•å•ä¸ªèŠ‚ç‚¹"""
        host, port = None, 443
        try:
            if node.startswith("vmess://"):
                content = node[8:]
                padding = len(content) % 4
                if padding:
                    content += '=' * (4 - padding)
                data = json.loads(base64.b64decode(content).decode('utf-8'))
                host = data.get("add", "")
                port = int(data.get("port", 443))
        except:
            pass
        
        if not host:
            return node, float('inf'), False
        
        timeout = random.uniform(Config.TEST_TIMEOUT_MIN, Config.TEST_TIMEOUT_MAX)
        start = time.time()
        success = False
        
        try:
            reader, writer = await asyncio.wait_for(
                asyncio.open_connection(host, port),
                timeout=timeout
            )
            writer.close()
            await writer.wait_closed()
            success = True
        except:
            success = False
        
        latency = (time.time() - start) * 1000 if success else float('inf')
        
        if use_king_system:
            king_system.update(node, latency, success)
        
        return node, latency, success
    
    semaphore = asyncio.Semaphore(min(Config.MAX_CONCURRENT_REQUESTS, 50))
    
    async def bounded_test(node):
        async with semaphore:
            return await test_node(node)
    
    tasks = [bounded_test(node) for node in nodes]
    results = await asyncio.gather(*tasks)
    
    alive = []
    for node, latency, success in results:
        if success and latency < Config.MAX_TEST_LATENCY:
            alive.append(node)
    
    king_node = None
    if use_king_system:
        # ğŸ†• ä½¿ç”¨æœ€ä½³èŠ‚ç‚¹ç‹ï¼ˆåŒ…æ‹¬å†å²å’Œå½“å‰ï¼‰
        best_king = king_system.get_best_king_overall()
        
        if best_king:
            king_node = best_king['node']
            if best_king.get('is_revived'):
                logging.info(f"[ğŸ‘‘] å·²é‡æ–°æ¿€æ´»å†å²èŠ‚ç‚¹ç‹: {best_king['node_id'][:8]} "
                            f"å»¶è¿Ÿ:{best_king['latency']:.1f}ms å¾—åˆ†:{best_king['score']:.1f}")
            else:
                logging.info(f"[ğŸ‘‘] ä½¿ç”¨æœ€ä½³èŠ‚ç‚¹ç‹: {best_king['node_id'][:8]} "
                            f"å»¶è¿Ÿ:{best_king['latency']:.1f}ms å¾—åˆ†:{best_king['score']:.1f}")
        else:
            # å›é€€åˆ°é€‰æ‹©æ–°çš„èŠ‚ç‚¹ç‹
            king_info = king_system.select_king()
            if king_info:
                king_node = king_info['node']
        
        # ç¡®ä¿èŠ‚ç‚¹ç‹åœ¨æœ€å‰é¢
        if king_node and king_node in alive:
            if king_node in alive:
                alive.remove(king_node)
            alive.insert(0, king_node)
        
        # æ—¥å¸¸æ£€æŸ¥å’Œä¿å­˜
        if random.random() < 0.3:
            king_system.daily_check()
        
        king_system.save()
        
        stats = king_system.stats()
        logging.info(f"[æµ‹é€Ÿ] {len(alive)}ä¸ªèŠ‚ç‚¹å­˜æ´»ï¼Œå¹³å‡å»¶è¿Ÿ:{stats['avg_latency']:.1f}ms")
    
    return alive[:Config.MAX_NODES], king_node

def get_today_date_str() -> str:
    """è·å–å½“å‰æ—¥æœŸçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²"""
    return datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')

def find_node_page_url(main_url: str) -> Optional[str]:
    """ä»ä¸»é¡µæŸ¥æ‰¾åŒ…å«å½“å¤©èŠ‚ç‚¹çš„é¡µé¢URL"""
    try:
        logging.info(f"æ­£åœ¨è®¿é—®ä¸»é¡µé¢: {main_url}")
        response = requests.get(main_url, headers=get_random_headers(), timeout=5)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        today = get_today_date_str()
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ‰“å°å‰10ä¸ªé“¾æ¥æ–‡æœ¬
        logging.info(f"ä»Šå¤©çš„æ—¥æœŸæ ¼å¼: {today}")
        logging.info("ä¸»é¡µé¢ä¸Šçš„éƒ¨åˆ†é“¾æ¥æ–‡æœ¬:")
        all_a_tags = soup.find_all('a', href=True)
        for i, a_tag in enumerate(all_a_tags[:10]):
            link_text = a_tag.get_text(strip=True)
            logging.info(f"é“¾æ¥{i+1}: {link_text}")
            # æ£€æŸ¥æ˜¯å¦åŒ…å«"å…è´¹"æˆ–"èŠ‚ç‚¹"ç­‰å…³é”®è¯
            if "å…è´¹" in link_text or "èŠ‚ç‚¹" in link_text:
                logging.info(f"æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„é“¾æ¥: {link_text} -> {a_tag['href']}")
        
        # å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"èŠ‚ç‚¹"çš„é“¾æ¥
        # ä¼˜å…ˆæŸ¥æ‰¾åŒ…å«å…·ä½“æ—¥æœŸå’ŒèŠ‚ç‚¹çš„é“¾æ¥
        specific_node_links = []
        general_node_links = []
        
        for a_tag in all_a_tags:
            link_text = a_tag.get_text(strip=True)
            href = a_tag['href']
            
            # è·³è¿‡å¯¼èˆªé“¾æ¥
            if link_text == "æ¯æ—¥å…è´¹èŠ‚ç‚¹" or link_text == "ç½‘ç«™é¦–é¡µ" or link_text == "ç§‘å­¦ä¸Šç½‘å®¢æˆ·ç«¯ä¸‹è½½":
                continue
                
            # åˆ†ç±»é“¾æ¥
            if (today in link_text and "èŠ‚ç‚¹" in link_text) or ("ä»Šæ—¥" in link_text and "èŠ‚ç‚¹" in link_text):
                specific_node_links.append((link_text, href))
            elif "å…è´¹" in link_text and "èŠ‚ç‚¹" in link_text:
                general_node_links.append((link_text, href))
            elif "v2ray" in link_text.lower() and "clash" in link_text.lower():
                general_node_links.append((link_text, href))
        
        # ä¼˜å…ˆè¿”å›ä»Šæ—¥èŠ‚ç‚¹é“¾æ¥
        if specific_node_links:
            link_text, href = specific_node_links[0]
            logging.info(f"æ‰¾åˆ°ä»Šæ—¥èŠ‚ç‚¹é“¾æ¥: {link_text} -> {href}")
            return href
        
        # å¦‚æœæ²¡æœ‰ä»Šæ—¥èŠ‚ç‚¹ï¼Œè¿”å›æœ€æ–°çš„å…è´¹èŠ‚ç‚¹é“¾æ¥
        if general_node_links:
            link_text, href = general_node_links[0]
            logging.info(f"æ‰¾åˆ°æœ€æ–°å…è´¹èŠ‚ç‚¹é“¾æ¥: {link_text} -> {href}")
            return href
        
        logging.warning("æœªæ‰¾åˆ°ä»Šæ—¥å…è´¹ç²¾é€‰èŠ‚ç‚¹é“¾æ¥")
    except requests.RequestException as e:
        logging.error(f"è®¿é—®ä¸»é¡µé¢å¤±è´¥: {e}")
    except Exception as e:
        logging.error(f"è§£æä¸»é¡µé¢å¤±è´¥: {e}")
    
    return None

def find_v2rayn_installation(base_dir: str = None) -> Optional[str]:
    """åœ¨ç³»ç»Ÿä¸ŠæŸ¥æ‰¾ v2rayN çš„å®‰è£…ç›®å½•"""
    default_paths = [
        os.path.join(os.environ.get('ProgramFiles', ''), 'v2rayN'),
        os.path.join(os.environ.get('ProgramFiles(x86)', ''), 'v2rayN'),
        os.path.expanduser('~\\AppData\\Local\\Programs\\v2rayN')
    ]
    
    search_paths = []
    if base_dir:
        search_paths.append(base_dir)
    search_paths.extend(default_paths)
    
    for path in search_paths:
        exe_path = os.path.join(path, 'v2rayN.exe')
        if os.path.exists(exe_path):
            return path
    
    for root, dirs, files in os.walk('d:\\', topdown=True):
        if 'v2rayN.exe' in files:
            return root
        if root.count(os.sep) >= 3:
            dirs[:] = []
    
    return None

def validate_v2rayn_installation() -> bool:
    """éªŒè¯v2rayNå®‰è£…æ˜¯å¦æ­£ç¡®"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    v2rayn_dir = find_v2rayn_installation(script_dir)
    
    if not v2rayn_dir:
        print("é”™è¯¯: æ‰¾ä¸åˆ° v2rayN å®‰è£…ç›®å½•")
        return False
    
    print(f"æ‰¾åˆ° v2rayN ç›®å½•: {v2rayn_dir}")
    
    config_path = get_config_path(v2rayn_dir)
    if not config_path:
        print("é”™è¯¯: æ‰¾ä¸åˆ° config.json æ–‡ä»¶")
        return False
    
    print(f"æ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_path}")
    
    exe_path = os.path.join(v2rayn_dir, 'v2rayN.exe')
    if not os.path.exists(exe_path):
        print("é”™è¯¯: æ‰¾ä¸åˆ°xray.exe ")
        return False
    
    print("æ‰€æœ‰å¿…è¦æ–‡ä»¶éªŒè¯é€šè¿‡")
    print(f"v2rayN.exe è·¯å¾„: {exe_path}")
    print(f"config.json è·¯å¾„: {config_path}")
    return True

def extract_node_url(node_page_url: str) -> Optional[str]:
    """ä»èŠ‚ç‚¹é¡µé¢æå–èŠ‚ç‚¹æ–‡ä»¶URL"""
    try:
        logging.info(f"æ­£åœ¨è®¿é—®èŠ‚ç‚¹é¡µé¢: {node_page_url}")
        response = requests.get(node_page_url, headers=get_random_headers(), timeout=5)
        response.raise_for_status()
        
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¹²å‡€çš„èŠ‚ç‚¹æ–‡ä»¶é“¾æ¥
        txt_pattern = re.compile(r'http[s]?://[^"\'<>\s]+\.(?:txt|yaml|yml)', re.IGNORECASE)
        txt_links = txt_pattern.findall(response.text)
        
        if txt_links:
            # ä¼˜å…ˆé€‰æ‹©.txté“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰åˆ™é€‰æ‹©.yamlæˆ–.ymlé“¾æ¥
            for link in txt_links:
                if link.lower().endswith('.txt'):
                    return link
            # å¦‚æœæ²¡æœ‰.txté“¾æ¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„é“¾æ¥
            return txt_links[0]
        
        logging.warning("æœªæ‰¾åˆ° .txt èŠ‚ç‚¹é“¾æ¥")
    except requests.RequestException as e:
        logging.error(f"è®¿é—®èŠ‚ç‚¹é¡µé¢å¤±è´¥: {e}")
    except Exception as e:
        logging.error(f"è§£æèŠ‚ç‚¹é¡µé¢å¤±è´¥: {e}")
    
    return None

@smart_retry(max_retries=3)
def download_nodes_file(node_url: str) -> (bool, List[str]):
    """ä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶å¹¶ä¿å­˜åˆ°æœ¬åœ°"""
    fake_logging()
    memory_optimizer.auto_cleanup()
    try:
        logging.info(f"[ğŸ”’] æ­£åœ¨ä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶: {node_url[:20]}...")
        headers = get_random_headers(stealth=True)
        
        time.sleep(random.uniform(0.5, 1.5))
        
        response = requests.get(node_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        content_length = len(response.text)
        logging.info(f"[ğŸ“¥] æˆåŠŸä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶ï¼Œå¤§å°: {content_length / 1024:.2f}KB")
        
        lines = response.text.strip().split('\n')
        
        unique_lines = []
        seen_node_identifiers = set()
        
        for line in lines:
            if not line.strip():
                continue
                
            node_identifier = None
            
            if line.startswith("vmess://"):
                try:
                    vmess_content = line[8:]
                    padding = len(vmess_content) % 4
                    if padding:
                        vmess_content += '=' * (4 - padding)
                    
                    vmess_json = json.loads(base64.b64decode(vmess_content).decode('utf-8', errors='ignore'))
                    address = vmess_json.get("add", "")
                    port = str(vmess_json.get("port", ""))
                    if address and port:
                        node_identifier = f"{address}:{port}"
                except Exception:
                    pass
            
            elif line.startswith("trojan://"):
                try:
                    pattern = r'trojan://[^@]+@([^:]+):(\d+)'
                    match = re.search(pattern, line)
                    if match:
                        address = match.group(1)
                        port = match.group(2)
                        node_identifier = f"{address}:{port}"
                except Exception:
                    pass
            
            elif line.startswith("ss://"):
                try:
                    ss_content = line[5:]
                    if '#' in ss_content:
                        ss_content = ss_content.split('#')[0]
                    padding = len(ss_content) % 4
                    if padding:
                        ss_content += '=' * (4 - padding)
                    
                    decoded = base64.b64decode(ss_content).decode('utf-8', errors='ignore')
                    pattern = r'[^@]+@([^:]+):(\d+)'
                    match = re.search(pattern, decoded)
                    if match:
                        address = match.group(1)
                        port = match.group(2)
                        node_identifier = f"{address}:{port}"
                except Exception:
                    pass
            
            if node_identifier and node_identifier not in seen_node_identifiers:
                seen_node_identifiers.add(node_identifier)
                unique_lines.append(line)
            elif not node_identifier and line not in unique_lines:
                unique_lines.append(line)
        
        unique_content = '\n'.join(unique_lines)
        
        if Config.ENABLE_NODE_FILTERING and len(unique_lines) > Config.MAX_NODES:
            if has_async and Config.ENABLE_SPEED_TEST:
                logging.info("[ğŸ§ ] æ­£åœ¨è¿›è¡Œæ™ºèƒ½èŠ‚ç‚¹æµ‹é€Ÿ...")
                import asyncio
                unique_lines, best_node = asyncio.run(benchmark_nodes_async(unique_lines))
            else:
                unique_lines = random.sample(unique_lines, Config.MAX_NODES)
        
        unique_content = '\n'.join(unique_lines)
        
        if len(unique_lines) < len(lines):
            removed_count = len(lines) - len(unique_lines)
            logging.info(f"[ğŸ§¹] èŠ‚ç‚¹å»é‡å®Œæˆï¼Œä»{len(lines)}ä¸ªèŠ‚ç‚¹ä¸­å»é™¤äº†{removed_count}ä¸ªé‡å¤/ä½è´¨é‡èŠ‚ç‚¹")
        
        nodes_path = get_nodes_path()
        
        time.sleep(random.uniform(0.1, 0.3))
        
        with open(nodes_path, "w", encoding="utf-8") as f:
            f.write(unique_content)
        
        logging.info(f"[âœ…] èŠ‚ç‚¹æ–‡ä»¶å·²ä¿å­˜åˆ°: {nodes_path}ï¼Œå…±{len(unique_lines)}ä¸ªèŠ‚ç‚¹")
        
        return True, unique_lines
    except requests.RequestException as e:
        logging.error(f"[âŒ] ä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
        raise
    except Exception as e:
        logging.error(f"[âŒ] ä¿å­˜èŠ‚ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
        return False, []

_global_connection_pool = None

def get_connection_pool() -> ConnectionPool:
    """è·å–å…¨å±€è¿æ¥æ± å®ä¾‹"""
    global _global_connection_pool
    if _global_connection_pool is None:
        _global_connection_pool = ConnectionPool(max_connections=Config.MAX_CONCURRENT_REQUESTS)
    return _global_connection_pool

async def download_nodes_file_async(node_url: str) -> bool:
    """å¼‚æ­¥ä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶å¹¶ä¿å­˜åˆ°æœ¬åœ°"""
    if not has_async:
        return download_nodes_file(node_url)
    
    fake_logging()
    try:
        logging.info(f"[âš¡] æ­£åœ¨å¼‚æ­¥ä¸‹è½½èŠ‚ç‚¹æ–‡ä»¶: {node_url[:20]}...")
        
        headers = get_random_headers(stealth=True)
        
        pool = get_connection_pool()
        session = await pool.acquire()
        try:
            async with session.get(node_url, headers=headers, timeout=5) as response:
                response.raise_for_status()
                content = await response.text()
        finally:
            pool.release()
        
        lines = content.strip().split('\n')
        
        unique_lines = []
        seen_node_identifiers = set()
        valid_protocols = ['vmess://', 'vless://', 'trojan://', 'shadowsocks://']
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if not any(line.startswith(protocol) for protocol in valid_protocols):
                continue
            
            if line not in unique_lines:
                unique_lines.append(line)
        
        if Config.ENABLE_NODE_FILTERING and has_async:
            unique_lines, best_node = await benchmark_nodes_async(unique_lines)
            if best_node:
                logging.info("[ğŸ†] å·²ç¡®å®šæœ€ä¼˜èŠ‚ç‚¹ï¼Œå°†åœ¨æ·»åŠ èŠ‚ç‚¹æ—¶è®¾ç½®ä¸ºé»˜è®¤èŠ‚ç‚¹")
        
        if has_async:
            nodes_path = get_nodes_path()
            async with aiofiles.open(nodes_path, 'w', encoding='utf-8') as f:
                await f.write('\n'.join(unique_lines))
        else:
            nodes_path = get_nodes_path()
            with open(nodes_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(unique_lines))
        
        del content, lines, seen_node_identifiers
        import gc
        gc.collect()
        
        logging.info(f"[âœ…] å¼‚æ­¥ä¸‹è½½å®Œæˆï¼Œä¿å­˜äº†{len(unique_lines)}ä¸ªèŠ‚ç‚¹")
        return True
    except Exception as e:
        logging.error(f"[âŒ] å¼‚æ­¥ä¸‹è½½å¤±è´¥: {e}")
        return False

def handle_unexpected_error(exctype, value, traceback):
    """å¤„ç†æœªæ•è·çš„å¼‚å¸¸ï¼Œç¡®ä¿ç¨‹åºä¼˜é›…é€€å‡º"""
    logging.error(f"[ğŸ’¥] å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {exctype.__name__}: {value}")
    logging.error("[ğŸ“] è¯¦ç»†é”™è¯¯å †æ ˆ:")
    import traceback
    traceback_str = ''.join(traceback.format_exception(exctype, value, traceback))
    logging.error(traceback_str)
    
    if 'gc' in sys.modules:
        import gc
        gc.collect()
    
    logging.critical("[ğŸ’€] ç¨‹åºå› æœªé¢„æœŸé”™è¯¯è€Œå´©æºƒ")

sys.excepthook = handle_unexpected_error

def main():
    """ç¨‹åºä¸»å…¥å£å‡½æ•° - é›†æˆèŠ‚ç‚¹ç‹æœºåˆ¶"""
    setup_logging()
    logging.info("å¼€å§‹è¿è¡Œ - èŠ‚ç‚¹ç‹æ®‹é…·æ·˜æ±°ç³»ç»Ÿ")
    
    king_system = None
    if Config.NODE_KING_ENABLED:
        king_system = NodeKingSystem()
        logging.info("[ç³»ç»Ÿ] èŠ‚ç‚¹ç‹æ®‹é…·æ·˜æ±°å·²å¯ç”¨")
        
        if Config.HISTORY_KING_ENABLED:
            logging.info("[ç³»ç»Ÿ] å†å²èŠ‚ç‚¹ç‹æœºåˆ¶å·²å¯ç”¨")
    
    v2rayn_available = validate_v2rayn_installation()
    if not v2rayn_available:
        logging.warning("[è­¦å‘Š] v2rayNå®‰è£…éªŒè¯å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
    
    if v2rayn_available and not is_v2rayn_running():
        if not start_v2rayn():
            logging.warning("[è­¦å‘Š] v2rayNå¯åŠ¨å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
    
    page_url = find_node_page_url(Config.MAIN_URL)
    if not page_url:
        logging.error("[é”™è¯¯] æœªæ‰¾åˆ°èŠ‚ç‚¹é¡µé¢")
        sys.exit(1)
    
    node_url = extract_node_url(page_url)
    if not node_url:
        logging.error("[é”™è¯¯] æœªæ‰¾åˆ°èŠ‚ç‚¹æ–‡ä»¶")
        sys.exit(1)
    
    success, raw_nodes = download_nodes_file(node_url)
    if not success:
        logging.error("[é”™è¯¯] ä¸‹è½½èŠ‚ç‚¹å¤±è´¥")
        sys.exit(1)
    
    logging.info(f"[ä¸‹è½½] å…±{len(raw_nodes)}ä¸ªèŠ‚ç‚¹")
    
    logging.info("[æµ‹é€Ÿ] å¼€å§‹èŠ‚ç‚¹æµ‹é€Ÿ")
    
    if has_async and Config.ENABLE_SPEED_TEST:
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        alive_nodes, king_node = loop.run_until_complete(
            enhanced_benchmark_nodes_async(raw_nodes, king_system)
        )
    else:
        alive_nodes = raw_nodes[:Config.MAX_NODES]
        king_node = None
    
    nodes_path = get_nodes_path()
    with open(nodes_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(alive_nodes))
    
    logging.info(f"[ä¿å­˜] {len(alive_nodes)}ä¸ªèŠ‚ç‚¹å·²ä¿å­˜")
    
    if king_system:
        # ğŸ†• æ˜¾ç¤ºå½“å‰æœ€ä½³èŠ‚ç‚¹ç‹ä¿¡æ¯
        best_king = king_system.get_best_king_overall()
        if best_king:
            king_type = "å†å²èŠ‚ç‚¹ç‹" if best_king.get('is_history') else "å½“å‰èŠ‚ç‚¹ç‹"
            king_status = "(é‡æ–°æ¿€æ´»)" if best_king.get('is_revived') else ""
            logging.info(f"[ğŸ‘‘] æœ€ä½³èŠ‚ç‚¹ç‹: {best_king['node_id'][:8]} {king_type}{king_status}")
            logging.info(f"      å¾—åˆ†:{best_king['score']:.1f} å»¶è¿Ÿ:{best_king['latency']:.1f}ms")
    
    if not add_nodes_to_mibei_group(king_node):
        logging.warning("[è­¦å‘Š] æ·»åŠ èŠ‚ç‚¹åˆ°åˆ†ç»„å¤±è´¥")
    
    if king_node:
        if set_best_node_as_default(king_node, "èŠ‚ç‚¹ç‹"):
            logging.info("[æˆåŠŸ] èŠ‚ç‚¹ç‹å·²è®¾ä¸ºé»˜è®¤èŠ‚ç‚¹")
    
    if update_v2rayn_subscription(node_url):
        logging.info("[æˆåŠŸ] è®¢é˜…å·²æ›´æ–°")
    
    if restart_v2rayn():
        logging.info("[æˆåŠŸ] v2rayNå·²é‡å¯")
    
    if king_system:
        stats = king_system.stats()
        logging.info(f"\n{'='*50}")
        logging.info("æœ€ç»ˆç»Ÿè®¡")
        logging.info(f"{'='*50}")
        logging.info(f"æ´»è·ƒèŠ‚ç‚¹: {stats['active_nodes']}ä¸ª")
        logging.info(f"èŠ‚ç‚¹ç‹: {stats['kings']}ä¸ª")
        logging.info(f"æ·˜æ±°èŠ‚ç‚¹: {stats['dead_nodes']}ä¸ª")
        logging.info(f"å†å²èŠ‚ç‚¹ç‹: {len(king_system.kings)}ä¸ª")
        logging.info(f"å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']:.1f}ms")
        logging.info(f"å¹³å‡æˆåŠŸç‡: {stats['avg_success']:.1%}")
        logging.info(f"{'='*50}")
    
    logging.info("ç¨‹åºè¿è¡Œå®Œæˆ")

def update_and_restart_if_needed():
    """æ›´æ–°èŠ‚ç‚¹å¹¶é‡å¯ v2rayN çš„ä¸»æµç¨‹"""
    node_page_url = find_node_page_url(Config.MAIN_URL)
    if not node_page_url:
        return
    
    node_url = extract_node_url(node_page_url)
    if not node_url:
        return
    
    success, best_node = download_nodes_file(node_url)
    if not success:
        return
    
    if not add_nodes_to_mibei_group(best_node):
        logging.warning("æ·»åŠ èŠ‚ç‚¹åˆ°ç±³è´åˆ†ç»„å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")
    
    if update_v2rayn_subscription(node_url):
        restart_v2rayn()

def generate_silent_bat_and_vbs(script_name: str = "v2ray_auto_updater.py", bat_name: str = "run_v2ray_silent.bat", vbs_name: str = "silent_runner.vbs"):
    """ç”Ÿæˆä¸€ä¸ª .bat å’Œ .vbs æ–‡ä»¶ç»„åˆæ¥å®ç° Python è„šæœ¬çš„é™é»˜è¿è¡Œ"""
    vbs_content = f'''Set WshShell = CreateObject("WScript.Shell")
WshShell.Run "python {os.path.join(Config.BASE_DIR, script_name)}", 0, False
'''
    
    vbs_path = os.path.join(Config.BASE_DIR, vbs_name)
    
    try:
        with open(vbs_path, "w", encoding="utf-8") as f:
            f.write(vbs_content)
        
        print(f"[âˆš] å·²ç”Ÿæˆé™é»˜è¿è¡Œ VBS æ–‡ä»¶: {vbs_path}")
    except Exception as e:
        print(f"[Ã—] ç”Ÿæˆ VBS æ–‡ä»¶å¤±è´¥: {e}")
    
    bat_content = f"""@echo off
REM ä½¿ç”¨ VBS è„šæœ¬åœ¨åå°è¿è¡Œ Python è„šæœ¬
start /min "" cscript "{vbs_path}"
exit
"""
    
    bat_path = os.path.join(Config.BASE_DIR, bat_name)
    
    try:
        with open(bat_path, "w", encoding="utf-8") as f:
            f.write(bat_content)
        
        print(f"[âˆš] å·²ç”Ÿæˆé™é»˜è¿è¡Œæ‰¹å¤„ç†æ–‡ä»¶: {bat_path}")
    except Exception as e:
        print(f"[Ã—] ç”Ÿæˆ .bat æ–‡ä»¶å¤±è´¥: {e}")

def run_script_no_window(script_path):
    """æ— çª—å£åŒ–è¿è¡Œ"""
    startupinfo = subprocess.STARTUPINFO()
    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
    
    process = subprocess.Popen(
        ["python", script_path],
        startupinfo=startupinfo,
        creationflags=subprocess.CREATE_NO_WINDOW
    )
    process.wait()

async def fetch_nodes_async():
    """å¼‚æ­¥è·å–èŠ‚ç‚¹"""
    try:
        node_page_url = find_node_page_url(Config.MAIN_URL)
        if not node_page_url:
            return False
        
        node_url = extract_node_url(node_page_url)
        if not node_url:
            return False
        
        return await download_nodes_file_async(node_url)
    except Exception as e:
        logging.error(f"[âŒ] å¼‚æ­¥è·å–èŠ‚ç‚¹å¤±è´¥: {e}")
        return False

async def benchmark_existing_nodes_async():
    """å¼‚æ­¥æµ‹é€Ÿç°æœ‰èŠ‚ç‚¹"""
    try:
        nodes_path = get_nodes_path()
        if not os.path.exists(nodes_path):
            return False
        
        async with aiofiles.open(nodes_path, 'r', encoding='utf-8') as f:
            content = await f.read()
        
        nodes = content.strip().split('\n')
        if nodes:
            _, best_node = await benchmark_nodes_async(nodes)
            if best_node:
                logging.info("[ğŸ†] å·²æ‰¾åˆ°æœ€ä¼˜èŠ‚ç‚¹ï¼Œæ­£åœ¨è®¾ç½®ä¸ºé»˜è®¤èŠ‚ç‚¹...")
                set_best_node_as_default(best_node)
            return True
        return False
    except Exception as e:
        logging.error(f"[âŒ] å¼‚æ­¥æµ‹é€Ÿå¤±è´¥: {e}")
        return False

async def monitor_system_resources_async():
    """å¼‚æ­¥ç›‘æ§ç³»ç»Ÿèµ„æº"""
    try:
        while True:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            
            if cpu_percent > 80 or memory_percent > 90:
                logging.warning(f"[âš ï¸] ç³»ç»Ÿèµ„æºè­¦å‘Š - CPU: {cpu_percent}%, å†…å­˜: {memory_percent}%")
            
            await asyncio.sleep(3)
            break
        return True
    except Exception as e:
        logging.error(f"[âŒ] å¼‚æ­¥ç›‘æ§ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")
        return False

async def fetch_nodes_async_wrapper():
    """è·å–èŠ‚ç‚¹çš„å¼‚æ­¥åŒ…è£…"""
    node_page_url = find_node_page_url(Config.MAIN_URL)
    if not node_page_url:
        return None
        
    node_url = extract_node_url(node_page_url)
    if not node_url:
        return None
        
    return await download_nodes_file_async(node_url)

async def benchmark_existing_nodes_async_wrapper():
    """æµ‹é€Ÿç°æœ‰èŠ‚ç‚¹çš„å¼‚æ­¥åŒ…è£…"""
    return True

async def monitor_system_resources_async_wrapper():
    """ç›‘æ§ç³»ç»Ÿèµ„æºçš„å¼‚æ­¥åŒ…è£…"""
    return True

def generate_random_ip() -> str:
    """é«˜æ•ˆç”ŸæˆéšæœºIPåœ°å€"""
    while True:
        ip = socket.inet_ntoa(struct.pack('>I', random.randint(1, 0xFFFFFFFF)))
        return ip

def generate_ip_range(start_ip: str, end_ip: str) -> List[str]:
    """ç”ŸæˆIPèŒƒå›´åˆ—è¡¨"""
    def ip_to_int(ip):
        return struct.unpack('>I', socket.inet_aton(ip))[0]
    
    def int_to_ip(ip_int):
        return socket.inet_ntoa(struct.pack('>I', ip_int))
    
    start = ip_to_int(start_ip)
    end = ip_to_int(end_ip)
    
    return [int_to_ip(ip) for ip in range(start, end + 1)]

async def scan_port_async(ip: str, port: int, timeout: float = 1.0) -> bool:
    """å¼‚æ­¥æ‰«æå•ä¸ªç«¯å£"""
    try:
        reader, writer = await asyncio.wait_for(
            asyncio.open_connection(ip, port),
            timeout=timeout
        )
        writer.close()
        await writer.wait_closed()
        return True
    except Exception:
        return False

async def scan_ip_async(ip: str, ports: List[int], timeout: float = 1.0) -> Dict[str, bool]:
    """å¼‚æ­¥æ‰«æå•ä¸ªIPçš„å¤šä¸ªç«¯å£"""
    results = {}
    tasks = []
    
    for port in ports:
        task = asyncio.create_task(scan_port_async(ip, port, timeout))
        tasks.append((port, task))
    
    for port, task in tasks:
        results[port] = await task
    
    return results

async def scan_network_async(
    ip_generator, 
    ports: List[int], 
    max_concurrent: int = 100,
    max_scans: int = 1000,
    timeout: float = 1.0
) -> Dict[str, Dict[str, bool]]:
    """å¼‚æ­¥æ‰«æç½‘ç»œ"""
    results = {}
    semaphore = asyncio.Semaphore(max_concurrent)
    tasks = []
    scanned_ips: Set[str] = set()
    
    async def scan_wrapper(ip: str):
        if ip in scanned_ips:
            return
        scanned_ips.add(ip)
        
        async with semaphore:
            try:
                scan_result = await scan_ip_async(ip, ports, timeout)
                if any(scan_result.values()):
                    results[ip] = scan_result
            except Exception as e:
                logging.debug(f"æ‰«æ {ip} å¤±è´¥: {e}")
    
    for _ in range(max_scans):
        ip = next(ip_generator)
        task = asyncio.create_task(scan_wrapper(ip))
        tasks.append(task)
    
    await asyncio.gather(*tasks, return_exceptions=True)
    
    return results

class PlatformAdapter:
    """è·¨å¹³å°é€‚é…å±‚ï¼Œæ”¯æŒWindows/Linux/macOS"""
    
    @staticmethod
    def get_platform() -> str:
        """è·å–å½“å‰å¹³å°"""
        if sys.platform.startswith('win'):
            return 'windows'
        elif sys.platform.startswith('linux'):
            return 'linux'
        elif sys.platform.startswith('darwin'):
            return 'macos'
        else:
            return 'unknown'
    
    @staticmethod
    def get_config_path(base_dir: str, config_name: str = "config.json", search_subdirs: bool = True) -> Optional[str]:
        """è·å–å¹³å°ç‰¹å®šçš„é…ç½®è·¯å¾„"""
        platform = PlatformAdapter.get_platform()
        
        platform_paths = {
            'windows': [
                os.path.join(base_dir, config_name),
                os.path.join(base_dir, 'binConfigs', config_name),
                os.path.join(os.path.expanduser("~"), "v2rayN", config_name),
            ],
            'linux': [
                os.path.join(base_dir, config_name),
                os.path.join(base_dir, '.config', config_name),
                os.path.join(os.path.expanduser("~"), '.config', 'v2rayn', config_name),
            ],
            'macos': [
                os.path.join(base_dir, config_name),
                os.path.join(base_dir, 'Library', 'Preferences', config_name),
                os.path.join(os.path.expanduser("~"), 'Library', 'Preferences', 'v2rayn', config_name),
            ]
        }
        
        default_paths = platform_paths.get(platform, [os.path.join(base_dir, config_name)])
        
        for path in default_paths:
            if os.path.exists(path):
                logging.debug(f"[âœ…] åœ¨å¹³å°ç‰¹å®šè·¯å¾„æ‰¾åˆ°é…ç½®æ–‡ä»¶: {path}")
                return path
        
        if search_subdirs:
            return find_config_file(config_name, [base_dir], recursive=True)
        
        return None
    
    @staticmethod
    def execute_command(cmd: str) -> Optional[str]:
        """æ‰§è¡Œå¹³å°ç‰¹å®šçš„å‘½ä»¤"""
        platform = PlatformAdapter.get_platform()
        
        try:
            if platform == 'windows':
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            else:
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True, executable='/bin/bash')
            
            if result.returncode == 0:
                return result.stdout.strip()
            return None
        except Exception as e:
            logging.error(f"æ‰§è¡Œå‘½ä»¤å¤±è´¥: {e}")
            return None

def ip_port_to_proxy_node(ip: str, port: int, protocol: str = 'vmess') -> str:
    """å°†IPå’Œç«¯å£è½¬æ¢ä¸ºä»£ç†èŠ‚ç‚¹å­—ç¬¦ä¸²"""
    if protocol == 'vmess':
        vmess_config = {
            "v": "2",
            "ps": f"æ‰«æèŠ‚ç‚¹_{ip}:{port}",
            "add": ip,
            "port": port,
            "id": "00000000-0000-0000-0000-000000000000",
            "aid": "0",
            "scy": "auto",
            "net": "tcp",
            "type": "none",
            "host": "",
            "path": "",
            "tls": "",
            "sni": "",
            "alpn": ""
        }
        vmess_json = json.dumps(vmess_config)
        vmess_b64 = base64.b64encode(vmess_json.encode('utf-8')).decode('utf-8')
        return f"vmess://{vmess_b64}"
    elif protocol == 'trojan':
        return f"trojan://password@{ip}:{port}#æ‰«æèŠ‚ç‚¹_{ip}:{port}"
    elif protocol == 'ss':
        ss_config = f"aes-256-gcm:password@{ip}:{port}"
        ss_b64 = base64.b64encode(ss_config.encode('utf-8')).decode('utf-8')
        return f"ss://{ss_b64}#æ‰«æèŠ‚ç‚¹_{ip}:{port}"
    else:
        return f"# ä¸æ”¯æŒçš„åè®®: {protocol}"

def parse_proxy_node(node: str) -> Dict[str, Any]:
    """è§£æä»£ç†èŠ‚ç‚¹å­—ç¬¦ä¸²"""
    info = {
        "protocol": "unknown",
        "ip": None,
        "port": None,
        "remarks": ""
    }
    
    try:
        if node.startswith("vmess://"):
            info["protocol"] = "vmess"
            vmess_content = node[8:]
            padding = len(vmess_content) % 4
            if padding:
                vmess_content += '=' * (4 - padding)
            vmess_json = json.loads(base64.b64decode(vmess_content).decode('utf-8', errors='ignore'))
            info["ip"] = vmess_json.get("add")
            info["port"] = vmess_json.get("port")
            info["remarks"] = vmess_json.get("ps", "")
        elif node.startswith("trojan://"):
            info["protocol"] = "trojan"
            pattern = r'trojan://[^@]+@([^:]+):(\d+)(?:#(.*))?'
            match = re.search(pattern, node)
            if match:
                info["ip"] = match.group(1)
                info["port"] = int(match.group(2))
                info["remarks"] = match.group(3) or ""
        elif node.startswith("ss://"):
            info["protocol"] = "ss"
            ss_content = node[5:]
            if '#' in ss_content:
                ss_content, _ = ss_content.split('#', 1)
            padding = len(ss_content) % 4
            if padding:
                ss_content += '=' * (4 - padding)
            decoded = base64.b64decode(ss_content).decode('utf-8', errors='ignore')
            pattern = r'[^:]+:[^@]+@([^:]+):(\d+)'
            match = re.search(pattern, decoded)
            if match:
                info["ip"] = match.group(1)
                info["port"] = int(match.group(2))
    except Exception as e:
        logging.debug(f"è§£æèŠ‚ç‚¹å¤±è´¥: {e}")
    
    return info

def merge_nodes(new_nodes: List[str], existing_nodes: List[str]) -> List[str]:
    """åˆå¹¶æ–°èŠ‚ç‚¹å’Œç°æœ‰èŠ‚ç‚¹ï¼Œå»é™¤é‡å¤é¡¹"""
    seen_identifiers = set()
    merged_nodes = []
    
    for node in existing_nodes:
        info = parse_proxy_node(node)
        if info["ip"] and info["port"]:
            identifier = f"{info['protocol']}_{info['ip']}_{info['port']}"
            seen_identifiers.add(identifier)
            merged_nodes.append(node)
    
    new_count = 0
    for node in new_nodes:
        info = parse_proxy_node(node)
        if info["ip"] and info["port"]:
            identifier = f"{info['protocol']}_{info['ip']}_{info['port']}"
            if identifier not in seen_identifiers:
                seen_identifiers.add(identifier)
                merged_nodes.append(node)
                new_count += 1
    
    logging.info(f"[âœ…] èŠ‚ç‚¹åˆå¹¶å®Œæˆï¼Œæ–°å¢ {new_count} ä¸ªèŠ‚ç‚¹ï¼Œæ€»è®¡ {len(merged_nodes)} ä¸ªèŠ‚ç‚¹")
    return merged_nodes

async def process_scan_results(scan_results: Dict[str, Dict[str, bool]]) -> List[str]:
    """å¤„ç†æ‰«æç»“æœå¹¶ç”Ÿæˆä»£ç†èŠ‚ç‚¹"""
    new_nodes = []
    
    for ip, ports in scan_results.items():
        for port, is_open in ports.items():
            if is_open:
                if port in [80, 8080, 8888]:
                    protocol = 'vmess'
                elif port in [443, 8443]:
                    protocol = 'trojan'
                elif port in [1080, 1081]:
                    protocol = 'ss'
                else:
                    protocol = 'vmess'
                
                node = ip_port_to_proxy_node(ip, port, protocol)
                new_nodes.append(node)
    
    logging.info(f"[ğŸ”] æ‰«æç»“æœå¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(new_nodes)} ä¸ªæ–°èŠ‚ç‚¹")
    return new_nodes

async def integrate_scan_results_with_existing() -> bool:
    """å°†æ‰«æç»“æœä¸ç°æœ‰èŠ‚ç‚¹æ•´åˆ"""
    try:
        logging.info("[âš¡] å¼€å§‹ç½‘ç»œæ‰«æ...")
        
        def ip_generator():
            while True:
                yield generate_random_ip()
        
        common_proxy_ports = [80, 443, 8080, 8443, 8888, 1080, 1081]
        
        scan_results = await scan_network_async(
            ip_generator(),
            common_proxy_ports,
            max_concurrent=Config.MAX_CONCURRENT_REQUESTS,
            max_scans=100,
            timeout=0.5
        )
        
        new_nodes = await process_scan_results(scan_results)
        
        nodes_path = get_nodes_path()
        existing_nodes = []
        if os.path.exists(nodes_path):
            with open(nodes_path, 'r', encoding='utf-8') as f:
                existing_nodes = [line.strip() for line in f if line.strip()]
        
        merged_nodes = merge_nodes(new_nodes, existing_nodes)
        
        with open(nodes_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(merged_nodes))
        
        logging.info(f"[âœ…] æ‰«æèŠ‚ç‚¹æ•´åˆå®Œæˆï¼ŒèŠ‚ç‚¹æ–‡ä»¶å·²æ›´æ–°")
        return True
        
    except Exception as e:
        logging.error(f"[âŒ] æ•´åˆæ‰«æç»“æœå¤±è´¥: {e}")
        return False

async def elite_main_async():
    """çœŸæ­£çš„å¼‚æ­¥é»‘å®¢æ¨¡å¼ - å®Œæ•´ç‰ˆ"""
    if not has_async:
        logging.warning("[âš ï¸] å¼‚æ­¥æ¨¡å—ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼")
        main()
        return
    
    setup_logging()
    logging.info("[âš¡] å¯åŠ¨å¼‚æ­¥é»‘å®¢æ¨¡å¼...")
    
    try:
        tasks = [
            asyncio.create_task(fetch_nodes_async_wrapper()),
            asyncio.create_task(benchmark_existing_nodes_async_wrapper()),
            asyncio.create_task(monitor_system_resources_async_wrapper())
        ]
        
        if Config.ENABLE_SCANNING:
            tasks.append(asyncio.create_task(integrate_scan_results_with_existing()))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = len([r for r in results if r and not isinstance(r, Exception)])
        logging.info(f"[âœ…] å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {success_count}/{len(tasks)} ä¸ªä»»åŠ¡æˆåŠŸ")
        
        return success_count > 0
        
    except Exception as e:
        logging.error(f"[âŒ] å¼‚æ­¥æ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
        return False

def ultimate_stealth():
    """ç»ˆæéšèº«æŠ€å·§ - å¢å¼ºç‰ˆ"""
    try:
        import ctypes
        kernel32 = ctypes.windll.kernel32
        
        kernel32.SetConsoleTitleW("svchost.exe")
        kernel32.ShowWindow(kernel32.GetConsoleWindow(), 0)
        
    except Exception as e:
        logging.debug(f"[ğŸ­] éšèº«æŠ€å·§éƒ¨åˆ†å¤±è´¥: {e}")
    
    fake_logging()
    
    stealth_messages = [
        "Windows Defender å®æ—¶ä¿æŠ¤æœåŠ¡è¿è¡Œä¸­",
        "ç³»ç»Ÿæ›´æ–°æœåŠ¡æ­£åœ¨æ£€æŸ¥æ›´æ–°",
        "åå°æ™ºèƒ½ä¼ è¾“æœåŠ¡è¿è¡Œæ­£å¸¸",
        "Windows æœç´¢ç´¢å¼•æœåŠ¡è¿è¡Œä¸­"
    ]
    logging.info(random.choice(stealth_messages))

if __name__ == "__main__":
    main()